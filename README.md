# TRANSFORMERS: FROM GROUND UP ğŸš€ğŸ§ 

Welcome to the **Transformers: From Ground Up** project! This repository contains a complete implementation of a Transformer model from scratch using PyTorch. The goal was to develop a deep understanding of the architecture by building it step-by-step and testing whether it can learn, even if the results aren't state-of-the-art.

---

## ğŸ“Œ Overview

This project implements:
- **Multi-Head Attention** ğŸ¤–
- **Positional Encoding** ğŸ“  
- **Encoder-Decoder Architecture** ğŸ”„  
- **Layer Normalization & Residual Connections** âš–ï¸  
- **Feed-Forward Networks** ğŸ§   
- **Custom Training Loop** ğŸ”„  

We trained the model on the **rajpurkar/squad** dataset for question answering. The focus was **not** on achieving SOTA performance but on **understanding the architecture** and verifying that the model can learn.

---

## ğŸ”§ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/transformers-from-scratch.git
   cd transformers-from-scratch
   ```

2. Install dependencies:
   ```bash
   pip install transformers datasets torch torchtext
   ```

---

## ğŸ“Š Results (Learning, Not Winning)

| Metric       | Training | Validation |
|--------------|----------|------------|
| Loss         | 4.21     | 4.85       |
| Accuracy     | 18%      | 12%        |

**Note:** These results show the model learns *something* - but don't expect miracles! The goal was validation, not performance. For better results you'd need:
- More layers and heads ğŸ§ 
- Longer training time â³  
- Proper QA evaluation metrics ğŸ“Š  

---

## ğŸ—ï¸ Code Structure

```
â”œâ”€â”€ Transformer.ipynb          # Main notebook with implementation
â”œâ”€â”€ README.md                  # This file
```

Key components:
- **`Embedding`**: Word embeddings + positional encoding
- **`MultiHeadAttention`**: Self/cross-attention mechanisms
- **`Encoder/Decoder`**: Stacks of attention + FFN layers
- **`ProjectionLayer`**: Final output layer

---

## ğŸ‹ï¸ Training

To train the QA model:
```python
transformer = Transformer.build_transformer(
    src_vocab_size=10000, 
    tgt_vocab_size=10000,
    src_seq_len=100, 
    tgt_seq_len=100
)
optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001)
criterion = nn.CrossEntropyLoss()

for epoch in range(5):
    train_epoch(transformer, train_loader, optimizer, criterion, device)
    evaluate(transformer, val_loader, criterion, device)
```

---

## ğŸ¤” Why This Exists

1. **Learn Transformers** by building every component
2. **Verify learning** on a real QA task
3. **Create reference code** for others

---

## ğŸ¯ Future Work

- Add span-based QA evaluation
- Implement attention masking
- Try different attention variants

---

## ğŸ“œ License

MIT - use freely but don't expect production-grade results!
